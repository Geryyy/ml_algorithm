{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed2924a",
   "metadata": {},
   "source": [
    "# Exercise Sheet: Neural Networks, Backpropagation, Gradient Boosting, and Data Uncertainty\n",
    "\n",
    "This notebook addresses the problems specified in the exercise sheet on Neural Networks, Backpropagation, Gradient Boosting, and Data Uncertainty. We will approach each problem step by step, implementing the required algorithms and analyzing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ddcbe",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3.1: Neural Network Configuration and Analysis\n",
    "\n",
    "### 3.1.1: XOR Network Configuration\n",
    "- Task: Configure a two-layer neural network to approximate a logical XOR.\n",
    "- Approach: Manually select activation functions (ReLU or linear) and set weights and biases.\n",
    "\n",
    "### 3.1.2: Derivatives of Loss Function\n",
    "- Task: Derive analytically the derivatives of the loss function with respect to various parameters.\n",
    "- Approach: Use the chain rule to find expressions for the derivatives. Explain the gradient flow during backpropagation.\n",
    "\n",
    "### 3.1.3: TensorFlow Keras Implementation\n",
    "- Task: Implement and train the neural network to approximate a logical AND.\n",
    "- Approach: Use TensorFlow Keras with a sigmoid activation function for the second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28791e5",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3.2: Learning Curves\n",
    "- Task: Implement networks and evaluate learning curves to analyze the training process.\n",
    "- Approach: Specific details will be addressed based on the figures and descriptions from the exercise sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf81bcc",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3.3: Gradient Boosting\n",
    "- Task: Perform tasks related to gradient boosting for regression and classification.\n",
    "- Approach: Use the provided CSV files and implement gradient boosting algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0f778",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3.4: Data Uncertainty and Gaussian Processes\n",
    "- Task: Handle data uncertainty, potentially using Gaussian processes.\n",
    "- Approach: Utilize the provided Python implementation and data files for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XOR Network Configuration\n",
    "\n",
    "# Define the weights and biases for the network\n",
    "# These values are placeholders and should be adjusted to meet the XOR approximation requirement\n",
    "weights = {\n",
    "    'layer0': [0.0, 0.0],  # weights for the first layer\n",
    "    'layer1': [0.0]        # weights for the second layer\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'layer0': [0.0, 0.0],  # biases for the first layer\n",
    "    'layer1': [0.0]        # biases for the second layer\n",
    "}\n",
    "\n",
    "# Define the activation functions for each layer (either 'relu' or 'linear')\n",
    "activation_functions = {\n",
    "    'layer0': ['relu', 'relu'],\n",
    "    'layer1': ['linear']\n",
    "}\n",
    "\n",
    "# Function to calculate the output of the network for a given input\n",
    "def neural_network_output(x):\n",
    "    # Implement the network computation here based on the weights, biases, and activation functions\n",
    "    pass  # Placeholder for the actual implementation\n",
    "\n",
    "# Example: Test the network with different inputs\n",
    "# print(neural_network_output([0, 0]))\n",
    "# print(neural_network_output([0, 1]))\n",
    "# print(neural_network_output([1, 0]))\n",
    "# print(neural_network_output([1, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9836530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Derivatives of Loss Function\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(y_true, y_pred):\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "# Function to calculate the derivative of the loss function with respect to weights and biases\n",
    "def loss_function_derivatives(y_true, y_pred, weights, biases):\n",
    "    # Implement the derivative calculations here\n",
    "    # The implementation depends on the structure of the network and the activation functions used\n",
    "    pass  # Placeholder for the actual implementation\n",
    "\n",
    "# Example: Test the derivative calculations\n",
    "# loss_derivative = loss_function_derivatives(y_true=1, y_pred=0.5, weights=weights, biases=biases)\n",
    "# print(loss_derivative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39fc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TensorFlow Keras Implementation for Logical AND\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(2, input_shape=(2,), activation='relu'),  # First layer with ReLU activation\n",
    "    Dense(1, activation='sigmoid')                  # Second layer with sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training data for logical AND\n",
    "X_train = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y_train = [0, 0, 0, 1]\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Extract and display the weights and biases\n",
    "weights, biases = model.layers[0].get_weights()\n",
    "print(\"Weights of first layer:\", weights)\n",
    "print(\"Biases of first layer:\", biases)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
